apiVersion: helm.toolkit.fluxcd.io/v2beta1
kind: HelmRelease
metadata:
  name: localai
  namespace: default
spec:
  interval: 1m
  chart:
    spec:
      chart: local-ai
      version: "2.1.2"
      sourceRef:
        kind: HelmRepository
        name: go-skynet
        namespace: flux-system
  values:
    replicaCount: 1
    deployment:
      image: quay.io/go-skynet/local-ai:latest
      env:
        threads: 14
        contextSize: 512
        modelsPath: "/models"
    fullnameOverride: "localai"
    resources:
      requests:
        cpu: 100m
        memory: 128Mi
    promptTemplates:
      ggml-gpt4all-j.tmpl: |
        The prompt below is a question to answer, a task to complete, or a conversation to respond to; decide which and write an appropriate response.
        ### Prompt:
        {{.Input}}
        ### Response:
    models:
      forceDownload: false
      list:
      - url: "https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGML/resolve/main/llama-2-7b-chat.ggmlv3.q4_0.bin"
        # basicAuth: base64EncodedCredentials
      persistence:
        pvc:
          enabled: true
          size: 6Gi
          accessModes:
          - ReadWriteOnce
          annotations: {}
          storageClass: longhorn
    service:
        # annotations:
        #             io.cilium/lb-ipam-ips: "192.168.1.21"
        type: ClusterIP
        ports:
          http:
            port: 80
    ingress:
        enabled: true
        ingressClassName: internal
        annotations:
          nginx.ingress.kubernetes.io/whitelist-source-range: |
            10.0.0.0/8,172.16.0.0/12,192.168.0.0/16
          hajimari.io/enable: "false"
        hosts:
        - host: &host localai.vanillax.me
          paths:
          - path: /
            pathType: Prefix
        tls:
        - hosts:
          - *host
    nodeSelector: {}
    tolerations: []
    affinity: {}


