# yaml-language-server: $schema=https://raw.githubusercontent.com/bjw-s/helm-charts/common-3.0.1/charts/library/common/values.schema.json
apiVersion: helm.toolkit.fluxcd.io/v2beta2
kind: HelmRelease
metadata:
  name: &app localai
  namespace: default
spec:
  interval: 30m
  chart:
    spec:
      # renovate: registryUrl=https://go-skynet.github.io/helm-charts/
      chart: local-ai
      version: 3.2.0
      sourceRef:
        kind: HelmRepository
        name: go-skynet
        namespace: flux-system

  maxHistory: 3

  install:
    createNamespace: true
    remediation:
      retries: 3

  upgrade:
    cleanupOnFail: true
    remediation:
      retries: 3

  uninstall:
    keepHistory: false

  values:
    replicaCount: 1
    fullnameOverride: "localai"
    deployment:
      image: quay.io/go-skynet/local-ai:v2.10.0-cublas-cuda12
      pullPolicy: IfNotPresent
      env:
        threads: 8
        context_size: 512
        rebuild: false
        debug: true
        OPENAI_API_KEY: 'sk-XXXXXXXXXXXXXXXXXXXX'
        NVIDIA_VISIBLE_DEVICES: all
        NVIDIA_DRIVER_CAPABILITIES: all
      modelsPath: "/models"
    resources:
      requests:
        cpu: 200m
        memory: 1Gi
        nvidia.com/gpu: 1
      limits:
        memory: 8Gi
        nvidia.com/gpu: 1

    promptTemplates:
      ggml-gpt4all-j.tmpl: |
        The prompt below is a question to answer, a task to complete, or a conversation to respond to; decide which and write an appropriate response.
        ### Prompt:
        {{.Input}}
        ### Response:
      llama-2-7b-chat.ggmlv3.q4_0.tmpl: |
        The prompt below is a question to answer, a task to complete, or a conversation to respond to; decide which and write an appropriate response.
        ### Prompt:
        {{.Input}}
        ### Response:
    models:
      forceDownload: false
      list:
      - url: "https://gpt4all.io/models/ggml-gpt4all-j.bin"
      - url: "https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/resolve/main/mistral-7b-instruct-v0.1.Q4_K_M.gguf"
      - url: "https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGML/resolve/main/llama-2-7b-chat.ggmlv3.q4_0.bin"
        # basicAuth: base64EncodedCredentials
      persistence:
        hostPath:
          enabled: false
          path: /models
        pvc:
          annotations: {}
          enabled: true
          storageClass: openebs-hostpath
          size: 100Gi



    service:
      annotations:
        io.cilium/lb-ipam-ips: 192.168.10.40
      type: LoadBalancer
      ports:
        http:
          port: 8080

    ingress:
      enabled: true
      className: internal
      annotations:
      hosts:
      - host: &host localai.vanillax.me
        paths:
        - path: /
          pathType: Prefix
      tls:
      - hosts:
        - *host

